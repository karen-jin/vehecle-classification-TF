{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow Lite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMtlfu-PTo5n",
        "colab_type": "text"
      },
      "source": [
        "![Colab](https://www.tensorflow.org/images/colab_logo_32px.png) [Run in Google Colab](https://colab.research.google.com/drive/19cEeW5Ub3ii3nv_aJ7SzGllUQUMznylb?usp=sharing) ![GitHub](https://www.tensorflow.org/images/GitHub-Mark-32px.png) [View source on GitHub](https://github.com/TanyaYu/Curriculum-Project/blob/master/module4/TensorFlow_Lite.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Xx5Ah0AMJp",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Lite\n",
        "\n",
        "TensorFlow Lite is a set of tools to help developers run TensorFlow models on *mobile*, *embedded*, and *IoT* devices. It enables on-device machine learning inference with low latency and a small binary size.\n",
        "\n",
        "TensorFlow Lite is designed to make it easy to perform machine learning on devices, \"at the edge\" of the network, instead of sending data back and forth from a server. For developers, performing machine learning on-device can help improve:\n",
        "\n",
        "- *Latency*: there's no round-trip to a server\n",
        "- *Privacy*: no data needs to leave the device\n",
        "- *Connectivity*: an Internet connection isn't required\n",
        "- *Power consumption*: network connections are power hungry\n",
        "\n",
        "TensorFlow Lite works with a huge range of devices, from tiny microcontrollers to powerful mobile phones.\n",
        "\n",
        "## Development workflow\n",
        "The workflow for using TensorFlow Lite involves the following steps:\n",
        "\n",
        "1. **Pick a model**\n",
        "\n",
        "  Bring your own TensorFlow model, find a model online, or pick a model from [Pre-trained models](https://www.tensorflow.org/lite/models) to drop in or retrain.\n",
        "\n",
        "2. **Convert the model**\n",
        "\n",
        "  If you're using a custom model, use the [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert/index) and a few lines of Python to convert it to the TensorFlow Lite format.\n",
        "\n",
        "3. **Deploy to your device**\n",
        "\n",
        "  Run your model on-device with the [TensorFlow Lite interpreter](https://www.tensorflow.org/lite/guide/inference), with APIs in many languages.\n",
        "\n",
        "4. **Optimize your model**\n",
        "\n",
        "  Use our [Model Optimization Toolkit](https://www.tensorflow.org/lite/performance/model_optimization) to reduce your model's size and increase its efficiency with minimal impact on accuracy.\n",
        "\n",
        "To learn more about using TensorFlow Lite in your project, see [Get started](https://www.tensorflow.org/lite/guide/get_started)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSiGcu7BFeus",
        "colab_type": "text"
      },
      "source": [
        "# Vehicles Classification Model\n",
        "In this section we are going to build and train neural netowrk to classify vehicles. Then, we will convert the model into TensorFlow Lite format to be able to deploy it in a mobile app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gki7TPSQKccC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Activation\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adamax, SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import get_file\n",
        "from os.path import join, dirname, basename\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import glob"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRt7dzeuFupS",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DHhhX6bFwil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://storage.googleapis.com/vehicle-dataset/vehicles_full.zip'\n",
        "path_to_zip =get_file('vehicles_full.zip', origin=url, extract=True)\n",
        "path = join(dirname(path_to_zip), 'vehicles_full')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S9u94IcF3dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = glob.glob(path + '/*/*', recursive=True)\n",
        "  \n",
        "X = files\n",
        "y = [basename(dirname(f)) for f in files]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = pd.DataFrame({\"filename\": X_train, \"class\": y_train}) \n",
        "test_df = pd.DataFrame({\"filename\": X_test, \"class\": y_test}) "
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqUyWOyOF6GU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_HEIGHT = 32 \n",
        "IMG_WIDTH = 64\n",
        "batch_size = 128\n",
        "class_names = ['Class1', 'Class2', 'Class3', 'Class4', 'Class5', 'Class6']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjgS1gdDF_4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "5f59be60-724a-4b21-c6c6-d0fd4b80a92e"
      },
      "source": [
        "image_generator = ImageDataGenerator(rescale=1./255,\n",
        "                                     width_shift_range=.15,\n",
        "                                     height_shift_range=.15,\n",
        "                                     brightness_range=(0.1,0.9),\n",
        "                                     zoom_range=0.3,\n",
        "                                     channel_shift_range=150,\n",
        "                                     horizontal_flip=True,\n",
        "                                     validation_split=0.2) \n",
        "\n",
        "train_data_gen = image_generator.flow_from_dataframe(dataframe=train_df,\n",
        "                                                     x_col=\"filename\",\n",
        "                                                     y_col=\"class\",\n",
        "                                                     subset=\"training\",\n",
        "                                                     shuffle=True,\n",
        "                                                     seed=42,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     classes=class_names,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode=\"categorical\")\n",
        "valid_data_gen = image_generator.flow_from_dataframe(dataframe=train_df,\n",
        "                                                     x_col=\"filename\",\n",
        "                                                     y_col=\"class\",\n",
        "                                                     subset=\"validation\",\n",
        "                                                     shuffle=True,\n",
        "                                                     seed=42,\n",
        "                                                     batch_size=batch_size,\n",
        "                                                     classes=class_names,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     class_mode=\"categorical\")\n",
        "\n",
        "test_image_generator = ImageDataGenerator(rescale=1./255) \n",
        "test_data_gen = test_image_generator.flow_from_dataframe(dataframe=test_df,\n",
        "                                                         x_col=\"filename\",\n",
        "                                                         y_col=\"class\",\n",
        "                                                         shuffle=False,\n",
        "                                                         classes=class_names,\n",
        "                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                         class_mode=\"categorical\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9437 validated image filenames belonging to 6 classes.\n",
            "Found 2359 validated image filenames belonging to 6 classes.\n",
            "Found 2950 validated image filenames belonging to 6 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 1 invalid image filename(s) in x_col=\"filename\". These filename(s) will be ignored.\n",
            "  .format(n_invalid, x_col)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0V31hNGE_Y",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnYfgxGGGlyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_top = ResNet50(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "x = GlobalAveragePooling2D()(model_top.output)  \n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128)(x)\n",
        "x = Dense(6)(x)\n",
        "x = Activation('softmax')(x)\n",
        "model = Model(model_top.input, x, name='resnet50')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghTo8wIjGsV8",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AGvRNTxHtne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4hwh21hG78_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss=CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lad_LXsoHABT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6653eb4f-aa0e-4d5f-da10-24f15a5ff110"
      },
      "source": [
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    validation_data=valid_data_gen,\n",
        "    epochs=100,\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "74/74 [==============================] - 16s 216ms/step - loss: 1.9418 - accuracy: 0.5501 - val_loss: 1.3126 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 1.6592 - accuracy: 0.5862 - val_loss: 1.1550 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.5784 - accuracy: 0.6186 - val_loss: 1.0612 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.3169 - accuracy: 0.6355 - val_loss: 1.2701 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.2846 - accuracy: 0.6488 - val_loss: 1.1551 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 1.1990 - accuracy: 0.6590 - val_loss: 1.1626 - val_accuracy: 0.6829 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.1852 - accuracy: 0.6630 - val_loss: 1.1658 - val_accuracy: 0.6838 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 1.1509 - accuracy: 0.6718 - val_loss: 1.0204 - val_accuracy: 0.6842 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.1307 - accuracy: 0.6750 - val_loss: 1.3547 - val_accuracy: 0.6753 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.0867 - accuracy: 0.6769 - val_loss: 2.1805 - val_accuracy: 0.6702 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 1.0501 - accuracy: 0.6819 - val_loss: 1.0435 - val_accuracy: 0.6969 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 1.0080 - accuracy: 0.6851 - val_loss: 0.8556 - val_accuracy: 0.7139 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.9632 - accuracy: 0.6930 - val_loss: 1.0187 - val_accuracy: 0.6867 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.9363 - accuracy: 0.6991 - val_loss: 0.8527 - val_accuracy: 0.7075 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.9230 - accuracy: 0.7021 - val_loss: 0.9501 - val_accuracy: 0.7177 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.8980 - accuracy: 0.7056 - val_loss: 0.9028 - val_accuracy: 0.6910 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.9154 - accuracy: 0.7042 - val_loss: 1.0636 - val_accuracy: 0.7054 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.8587 - accuracy: 0.7088 - val_loss: 1.6834 - val_accuracy: 0.6956 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "74/74 [==============================] - ETA: 0s - loss: 0.8338 - accuracy: 0.7162\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.8338 - accuracy: 0.7162 - val_loss: 2.3282 - val_accuracy: 0.6969 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7947 - accuracy: 0.7267 - val_loss: 0.7494 - val_accuracy: 0.7304 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7609 - accuracy: 0.7320 - val_loss: 0.8711 - val_accuracy: 0.7334 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.7730 - accuracy: 0.7320 - val_loss: 0.7145 - val_accuracy: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7670 - accuracy: 0.7337 - val_loss: 0.7045 - val_accuracy: 0.7452 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.7572 - accuracy: 0.7366 - val_loss: 0.7624 - val_accuracy: 0.7478 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.7441 - accuracy: 0.7385 - val_loss: 0.7174 - val_accuracy: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7414 - accuracy: 0.7386 - val_loss: 0.7501 - val_accuracy: 0.7478 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.7243 - accuracy: 0.7428 - val_loss: 0.7135 - val_accuracy: 0.7423 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "74/74 [==============================] - 15s 202ms/step - loss: 0.7292 - accuracy: 0.7406 - val_loss: 0.6814 - val_accuracy: 0.7503 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "74/74 [==============================] - 15s 206ms/step - loss: 0.7350 - accuracy: 0.7406 - val_loss: 0.6747 - val_accuracy: 0.7478 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.7340 - accuracy: 0.7453 - val_loss: 0.8033 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.7274 - accuracy: 0.7435 - val_loss: 0.7149 - val_accuracy: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7156 - accuracy: 0.7391 - val_loss: 0.7136 - val_accuracy: 0.7512 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.7218 - accuracy: 0.7488 - val_loss: 0.6962 - val_accuracy: 0.7575 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "74/74 [==============================] - ETA: 0s - loss: 0.7203 - accuracy: 0.7423\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.7203 - accuracy: 0.7423 - val_loss: 0.7657 - val_accuracy: 0.7490 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "74/74 [==============================] - 15s 206ms/step - loss: 0.7044 - accuracy: 0.7467 - val_loss: 0.6590 - val_accuracy: 0.7647 - lr: 1.0000e-06\n",
            "Epoch 36/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7049 - accuracy: 0.7431 - val_loss: 0.6647 - val_accuracy: 0.7537 - lr: 1.0000e-06\n",
            "Epoch 37/100\n",
            "74/74 [==============================] - 15s 203ms/step - loss: 0.7013 - accuracy: 0.7452 - val_loss: 0.7081 - val_accuracy: 0.7558 - lr: 1.0000e-06\n",
            "Epoch 38/100\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.7067 - accuracy: 0.7408 - val_loss: 0.7207 - val_accuracy: 0.7512 - lr: 1.0000e-06\n",
            "Epoch 39/100\n",
            "74/74 [==============================] - 15s 207ms/step - loss: 0.7061 - accuracy: 0.7490 - val_loss: 0.6817 - val_accuracy: 0.7643 - lr: 1.0000e-06\n",
            "Epoch 40/100\n",
            "74/74 [==============================] - ETA: 0s - loss: 0.7108 - accuracy: 0.7429\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "74/74 [==============================] - 15s 206ms/step - loss: 0.7108 - accuracy: 0.7429 - val_loss: 0.7058 - val_accuracy: 0.7537 - lr: 1.0000e-06\n",
            "Epoch 41/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.6996 - accuracy: 0.7407 - val_loss: 0.7344 - val_accuracy: 0.7596 - lr: 1.0000e-07\n",
            "Epoch 42/100\n",
            "74/74 [==============================] - 15s 204ms/step - loss: 0.6992 - accuracy: 0.7477 - val_loss: 0.6943 - val_accuracy: 0.7575 - lr: 1.0000e-07\n",
            "Epoch 43/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.6969 - accuracy: 0.7463 - val_loss: 0.7006 - val_accuracy: 0.7609 - lr: 1.0000e-07\n",
            "Epoch 44/100\n",
            "74/74 [==============================] - 15s 205ms/step - loss: 0.7013 - accuracy: 0.7468 - val_loss: 0.7126 - val_accuracy: 0.7541 - lr: 1.0000e-07\n",
            "Epoch 45/100\n",
            "74/74 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.7479\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "74/74 [==============================] - 15s 206ms/step - loss: 0.7029 - accuracy: 0.7479 - val_loss: 0.6950 - val_accuracy: 0.7503 - lr: 1.0000e-07\n",
            "Epoch 00045: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGv0n-BWGtnX",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8844WwaHWXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ff623a6-0e98-42f4-9227-c91f3cec65a5"
      },
      "source": [
        "test_eval = model.evaluate_generator(test_data_gen)\n",
        "print('Test Accuracy      : %1.2f%%     Test loss      : %1.6f' % (test_eval[1]*100, test_eval[0]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy      : 77.02%     Test loss      : 0.618203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjZmvCnfGqKv",
        "colab_type": "text"
      },
      "source": [
        "#Save the Model in TFLite Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaHvRpje-S41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.lite as lite\n",
        "from tensorflow import TensorSpec, function"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVe_otsz_G4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b7aa580-02a0-41e6-c7a8-08c8e10c4be9"
      },
      "source": [
        "!mkdir \"tflite_models\""
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘tflite_models’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOjZhXHj_Icz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the concrete function from the Keras model.\n",
        "run_model = function(lambda x : model(x))\n",
        "\n",
        "# Save the concrete function.\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwRVWuFd82Lm",
        "colab_type": "text"
      },
      "source": [
        "## Standard TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQK27_Lh9Mnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4ba5160c-44ae-4199-dc23-efd7ff156086"
      },
      "source": [
        "TFLITE_MODEL = \"tflite_models/vehicles.tflite\"\n",
        "\n",
        "# Convert the model to standard TensorFlow Lite model\n",
        "converter = lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converted_tflite_model = converter.convert()\n",
        "open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94997908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-lCT-od9NG1",
        "colab_type": "text"
      },
      "source": [
        "## Quantized Model\n",
        "Post-training quantization is a conversion technique that can reduce model size while also improving CPU and hardware accelerator latency, with little degradation in model accuracy. You can quantize an already-trained float TensorFlow model when you convert it to TensorFlow Lite format using the TensorFlow Lite Converter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhwtKpHKKZ4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9a697a5-fe82-45ad-8662-49d3bdcfe6c2"
      },
      "source": [
        "TFLITE_QUANT_MODEL = \"tflite_models/vehicles_quant.tflite\"\n",
        "\n",
        "# Convert the model to quantized version with post-training quantization\n",
        "converter = lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_quant_model = converter.convert()\n",
        "open(TFLITE_QUANT_MODEL, \"wb\").write(tflite_quant_model)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23848776"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NirohgSu-6-r",
        "colab_type": "text"
      },
      "source": [
        "## Convert from Saved Model or from Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKQqY9e1_x6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting a SavedModel to a TensorFlow Lite model.\n",
        "converter = lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Converting a tf.Keras model to a TensorFlow Lite model.\n",
        "converter = lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVrYv2FAOKz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}